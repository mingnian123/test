All experiments are conducted on a Windows 10 OS with an NVIDIA GeForce RTX 3060 GPU. The deep learning environment is built upon PyTorch 2.4.1 and CUDA 11.8, with all models implemented using Python 3.9.
In Stage I training, the AdamW optimizer is used along with a cosine annealing learning rate schedule to optimize the network by minimizing the cross-entropy loss function. The configuration includes a weight decay 1e−4, an initial learning rate of 5e−4, and the momentum parameters β = (0.9, 0.999). Training was conducted with a batch size of 32 for 150 epochs. In Stage II, a composite loss function was adopted for optimization, with a batch size of 16 and 200 training epochs. All other training settings remain consistent with those in Stage I.
